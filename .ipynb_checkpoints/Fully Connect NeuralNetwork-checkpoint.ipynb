{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from layers import *\n",
    "from gradientCheck import *\n",
    "from relativeError import *\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度检查： 看affine_backward是否实现正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.720377666\n",
      "162.720377669\n",
      "dX relative error: 0.000000\n",
      "dW relative error: 0.000000\n",
      "db relative error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "X = np.random.rand(60).reshape(3, 4, 5)\n",
    "\n",
    "W = np.random.randn(20, 5)\n",
    "\n",
    "b = np.random.rand(5)\n",
    "\n",
    "out, cache = affine_forward(X, W, b)\n",
    "\n",
    "dout = 10 * np.random.randn(*out.shape)\n",
    "\n",
    "f_X = lambda x: affine_forward(x, W, b)[0]\n",
    "\n",
    "f_W = lambda w: affine_forward(X, w, b)[0]\n",
    "\n",
    "f_b = lambda b: affine_forward(X, W, b)[0]\n",
    "\n",
    "dX_num = numerical_gradient(f_X, X, dout)\n",
    "\n",
    "dW_num = numerical_gradient(f_W, W, dout)\n",
    "\n",
    "db_num = numerical_gradient(f_b, b, dout)\n",
    "print(np.sum(dX_num))\n",
    "\n",
    "dX, dW, db = affine_backward(dout, cache)\n",
    "\n",
    "print(np.sum(dX))\n",
    "\n",
    "\n",
    "print('dX relative error: %f'%(relative_error(dX, dX_num)))\n",
    "\n",
    "print('dW relative error: %f'%(relative_error(dW, dW_num)))\n",
    "\n",
    "print('db relative error: %f'%(relative_error(db, db_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度检查： 看relu_backward是否实现正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08706084 -0.39078755 -0.53630667 -0.0694466 ]\n",
      " [ 1.19596293 -1.23569345 -0.53806511 -0.6078434 ]\n",
      " [ 0.92544992  0.28567297 -0.5000103  -0.88501449]]\n",
      "[[-0.75379626  0.          0.         -0.        ]\n",
      " [-0.93293922  0.          0.          0.        ]\n",
      " [-2.07169044 -0.26723653  0.         -0.        ]]\n",
      "[[-0.75379626  0.          0.          0.        ]\n",
      " [-0.93293922  0.          0.          0.        ]\n",
      " [-2.07169044 -0.26723653  0.          0.        ]]\n",
      "dX relative error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(3, 4)\n",
    "\n",
    "print(X)\n",
    "\n",
    "out, cache = relu_forward(X)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dX = relu_backward(dout, cache)\n",
    "\n",
    "print(dX)\n",
    "\n",
    "f = lambda x: relu_forward(x)[0]\n",
    "\n",
    "dX_num = numerical_gradient(f, X, dout)\n",
    "\n",
    "print(dX_num)\n",
    "\n",
    "print('dX relative error: %f'%(relative_error(dX, dX_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 梯度检查： 看affine_relu_backward是否实现正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "X = np.random.rand(60).reshape(3, 4, 5)\n",
    "\n",
    "W = np.random.randn(20, 5)\n",
    "\n",
    "b = np.random.rand(5)\n",
    "\n",
    "out, cache = affine_relu_forward(X, W, b)\n",
    "\n",
    "dout = 10 * np.random.randn(*out.shape)\n",
    "\n",
    "f_X = lambda x: affine_relu_forward(x, W, b)[0]\n",
    "\n",
    "f_W = lambda w: affine_relu_forward(X, w, b)[0]\n",
    "\n",
    "f_b = lambda b: affine_relu_forward(X, W, b)[0]\n",
    "\n",
    "dX_num = numerical_gradient(f_X, X, dout)\n",
    "\n",
    "dW_num = numerical_gradient(f_W, W, dout)\n",
    "\n",
    "db_num = numerical_gradient(f_b, b, dout)\n",
    "print(np.sum(dX_num))\n",
    "\n",
    "dX, dW, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "print(np.sum(dX))\n",
    "\n",
    "\n",
    "print('dX relative error: %f'%(relative_error(dX, dX_num)))\n",
    "\n",
    "print('dW relative error: %f'%(relative_error(dW, dW_num)))\n",
    "\n",
    "print('db relative error: %f'%(relative_error(db, db_num)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
